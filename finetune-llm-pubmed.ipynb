{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69e061aa-8079-4857-bbf6-330f97f263d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fine Tune LLM (On Text Classification Data)\n",
    "\n",
    "We'll walk through prepping data and fine-tuning a model in this notebook. Databricks makes it easy to fine tune a model with a few lines of code. We will be working with a text classification dataset: the [ml4pubmed dataset from hugging face](https://huggingface.co/datasets/ml4pubmed/pubmed-text-classification-cased) which contains text from different sections of scientific research articles.\n",
    "\n",
    "This notebook is the first in a series to showcase the performance gains provided by combining fine-tuning and prompt optimization. See this article for more [detail](https://www.databricks.com/blog/building-state-art-enterprise-agents-90x-cheaper-automated-prompt-optimization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d9b82d23-e17f-4d34-917e-f339b6b8c0d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Packages"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks_genai\n",
    "%pip install databricks-sdk\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c77a040d-77ad-41d2-b86e-866ff475f4d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Packages"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.model_training import foundation_model as fm\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import mlflow\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df19765-429b-4675-8543-1ca0751e14a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set Variables & Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2973b5e-5f8e-48fb-9c67-20e71e77e5cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Variables"
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"megan_fang_demos\"\n",
    "schema = \"llm_opt\"\n",
    "model_name = \"cc-meta-llama-3-1-8b-instruct\"\n",
    "registered_model_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "dataset = \"ml4pubmed/pubmed-text-classification-cased\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/Volumes/megan_fang_demos/llm_opt/datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f1093ad-bd0a-4689-b6e9-77e171a18ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's include a helpful system prompt for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b049ec49-8e55-413b-b946-729eab6110d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set System Prompt"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Given a text field containing a sentence from a research paper abstract, classify it into one of the following categories and output only the category label:\n",
    "\n",
    "**Categories:**\n",
    "- BACKGROUND: Contextual information, established knowledge, problem statements, or general facts that set up the research context (e.g., \"Although opioids are effective treatments for postoperative pain, they contribute to the delayed recovery of gastrointestinal function.\")\n",
    "- OBJECTIVE: Research aims, goals, purposes, hypotheses, or what the study was designed to investigate (e.g., \"This study was designed to assess...\", \"The aim was to investigate...\", \"To determine whether...\")\n",
    "- METHODS: Descriptions of experimental procedures, data collection methods, study protocols, analytical approaches, statistical methods, or control group descriptions (e.g., \"Blood samples were collected...\", \"This fifth group served as a control...\", \"mean (+/- SD) was used\")\n",
    "- RESULTS: Findings, outcomes, statistical data, success rates, observed effects, or factual outcomes from the study (e.g., \"The success rate was 70.4%...\", \"Follow-up coronary angiography was performed in 108 patients...\", \"a significant improvement in terms of OS (p = 0.02)\")\n",
    "- CONCLUSIONS: Final interpretations, implications, recommendations, suggestions, or what the authors conclude from their findings. These often contain interpretive language and speculation (e.g., \"From this finding, we conclude that...\", \"may be associated with...\")\n",
    "\n",
    "**Critical Classification Guidelines:**\n",
    "\n",
    "1. **OBJECTIVE vs BACKGROUND**: \n",
    "    - Sentences describing what a study \"was designed to,\" \"aimed to,\" \"evaluated whether,\" or starting with \"To determine/investigate\" are OBJECTIVE, not BACKGROUND\n",
    "    - \"There is a need to...\" statements are OBJECTIVE (expressing research purpose), not BACKGROUND\n",
    "    - Study purpose statements are always OBJECTIVE regardless of their position in the abstract\n",
    "\n",
    "2. **RESULTS vs CONCLUSIONS**: \n",
    "    - RESULTS report factual findings, statistical outcomes, and observed data without interpretation\n",
    "    - CONCLUSIONS contain interpretive statements with words like \"may be,\" \"suggest,\" \"conclude,\" \"might,\" or implications drawn from results\n",
    "    - Statements with \"evidence suggesting\" are CONCLUSIONS, not BACKGROUND\n",
    "\n",
    "3. **Key Phrases for OBJECTIVE**: \n",
    "    - \"There is a need to...\"\n",
    "    - \"This study evaluated whether...\"\n",
    "    - \"was designed to assess...\"\n",
    "    - \"The aim was to...\"\n",
    "    - \"To determine...\"\n",
    "    - \"This clinical study was designed to assess whether...\"\n",
    "\n",
    "4. **Key Phrases for CONCLUSIONS**: \n",
    "    - \"we conclude that...\"\n",
    "    - \"may be associated with...\"\n",
    "    - \"might benefit from...\"\n",
    "    - \"suggest that...\"\n",
    "    - \"evidence suggesting...\"\n",
    "    - Any speculative or interpretive language\n",
    "\n",
    "5. **METHODS indicators**: \n",
    "    - Descriptions of procedures, controls, statistical approaches\n",
    "    - Data collection timing and methods\n",
    "    - Analytical methods and statistical significance thresholds\n",
    "    - Control group descriptions\n",
    "\n",
    "6. **Focus on primary purpose**: Classify based on the main function of the sentence in the research narrative, not just its position in the abstract.\n",
    "\n",
    "**Common Misclassification Patterns to Avoid:**\n",
    "- Study objectives stated as \"This study was designed to...\" should be OBJECTIVE, not BACKGROUND\n",
    "- \"There is a need to identify...\" statements are OBJECTIVE, not BACKGROUND\n",
    "- Statements with \"may be associated with\" or similar speculative language are CONCLUSIONS, not RESULTS\n",
    "- \"Evidence suggesting\" statements are CONCLUSIONS, not BACKGROUND\n",
    "- Research aims are always OBJECTIVE regardless of how they are phrased\n",
    "\n",
    "**Classification Strategy:**\n",
    "1. Look for explicit objective markers first (study aims, purposes, \"to determine\")\n",
    "2. Check for speculative/interpretive language indicating CONCLUSIONS\n",
    "3. Identify factual data reporting for RESULTS\n",
    "4. Look for procedural descriptions for METHODS\n",
    "5. Default to BACKGROUND only for established knowledge or context-setting information\n",
    "\n",
    "Based on the above, categorize the following sentence and output only the category label (BACKGROUND, OBJECTIVE, METHODS, RESULTS, or CONCLUSIONS) without any additional text or explanation : \\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60b001b2-7c83-435d-9829-a496d63be342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Base Model\n",
    "\n",
    "Let's test the performance of our LLM before fine-tuning to get a baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ff205b-4925-4dd6-b27f-e2d55a2f2f8d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inference With Base Model"
    }
   },
   "outputs": [],
   "source": [
    "test_table_name = \"hf_pubmed_test\"\n",
    "pred = spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            ai_query('databricks-meta-llama-3-1-8b-instruct', concat('{system_prompt}', prompt)) AS prediction,\n",
    "            response,\n",
    "            prompt\n",
    "        FROM {catalog}.{schema}.{test_table_name}\n",
    "        LIMIT 10\"\"\")\n",
    "display(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "641fa875-7c09-4698-9768-51114cb136a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get Baseline Accuracy"
    }
   },
   "outputs": [],
   "source": [
    "total_count = pred.count()\n",
    "correct_predictions = pred.filter(col(\"prediction\") == col(\"response\")).count()\n",
    "overall_accuracy = correct_predictions / total_count\n",
    "\n",
    "print(f\"Total samples: {total_count}\")\n",
    "print(f\"Baseline Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70076b44-6379-4a75-b98d-4ddaec743567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e45fbf-1559-48c1-8847-19f99e587d36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function To Load Hugging Face Dataset"
    }
   },
   "outputs": [],
   "source": [
    "def load_hf_dataset(dataset: str, split: str=\"train\", include_cols: list[str]=['prompt', 'response'], rename_cols: Dict | None = None, register_to_uc: boolean=False, catalog: str=catalog, schema: schema=schema, table_name: str=train_table_name) -> SparkDataframe:\n",
    "  \"\"\"\n",
    "  Load dataset from hugging face and converts to spark dataframe. Optionally register to Unity Catalog.\n",
    "\n",
    "  Args:\n",
    "    dataset (str): Hugging Face dataset name\n",
    "    split (str): Dataset split to load\n",
    "    include_cols (list): Columns to include in the final dataframe\n",
    "    rename_cols (dict): Dictionary of column names to rename\n",
    "    register_to_uc (bool): Whether to register the dataframe to Unity Catalog\n",
    "    catalog (str): Unity Catalog catalog name\n",
    "    schema (str): Unity Catalog schema name\n",
    "    table_name (str): Unity Catalog table name\n",
    "  Returns:\n",
    "    Spark DataFrame: Converted dataframe\n",
    "  \"\"\"\n",
    "  # Load hugging face dataset\n",
    "  hf_df = load_dataset(\n",
    "      path=dataset,\n",
    "      split=split\n",
    "  )\n",
    "\n",
    "  # Convert to pandas then spark dataframe\n",
    "  pd_df = hf_df.to_pandas()[include_cols].dropna()\n",
    "  hf_spark = spark.createDataFrame(pd_df)\n",
    "\n",
    "  if rename_cols is not None:\n",
    "    hf_spark = hf_spark.withColumnRenamed(rename_cols[\"prompt\"], \"prompt\").withColumnRenamed(rename_cols[\"response\"], \"response\")\n",
    "\n",
    "  # Optionally register to unity catalog\n",
    "  if register_to_uc:\n",
    "    hf_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "\n",
    "  return hf_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98cd0858-da7f-4368-a3a1-249332777dc5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Train Dataset"
    }
   },
   "outputs": [],
   "source": [
    "train_table_name = \"hf_pubmed_train\"\n",
    "rename_cols = {\"prompt\": \"description\", \"response\": \"target\"}\n",
    "\n",
    "hf_train = load_hf_dataset(dataset=dataset, include_cols=['target', 'description'], rename_cols=rename_cols, register_to_uc=True, table_name=train_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6632885e-26c0-4599-9d7e-b835a1430b6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Eval Dataset"
    }
   },
   "outputs": [],
   "source": [
    "eval_table_name = \"hf_pubmed_eval\"\n",
    "hf_eval = load_hf_dataset(dataset=dataset, split=\"validation\", include_cols=['target', 'description'], rename_cols=rename_cols, register_to_uc=True, table_name=eval_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b22c7c-9119-4807-96d8-794c6c73e7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transform Data For Chat Completion\n",
    "\n",
    "Databricks fine-tuning supports a few different task types:\n",
    "\n",
    "- **Chat completion**: Train your model on chat logs between a user and an AI assistant. The text is automatically formatted into the appropriate format for the specific model. \n",
    "- **Instruction fine-tuning**: Train your model on structured prompt-response data. Use this to adapt your model to a new task, change its response style, or add instruction-following capabilities. This task does not automatically apply any formatting to your data and is only recommended when custom data formatting is required.\n",
    "\n",
    "I found chat completion to perform better than instruction fine-tuning for this task, so let's format our data for chat completion. \n",
    "\n",
    "Chat completion requires a list of role and prompt, following the OpenAI standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "882b71c8-0263-48df-adc0-f3daaacf084c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example Format for Training Data"
    }
   },
   "outputs": [],
   "source": [
    "[\n",
    "  {\"role\": \"system\", \"content\": \"[system prompt]\"},\n",
    "  {\"role\": \"user\", \"content\": \"Here is a documentation page:[RAG context]. Based on this, answer the following question: [user question]\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"[answer]\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9819877-010b-4a81-9a4e-06022cedf4a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transform Train Data"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE hf_pubmed_train_chat_complete AS\n",
    "SELECT \n",
    "    ARRAY(\n",
    "        STRUCT('user' AS role, CONCAT('{system_prompt}', '\\n', prompt) AS content),\n",
    "        STRUCT('assistant' AS role, response AS content)\n",
    "    ) AS messages\n",
    "FROM hf_pubmed_train;\n",
    "\"\"\")\n",
    "\n",
    "spark.table('hf_pubmed_train_chat_complete').limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516548db-cc1b-4ca4-8f20-6fa5fe5ad0b3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transform Eval Data"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE hf_pubmed_eval_chat_complete AS\n",
    "SELECT \n",
    "    ARRAY(\n",
    "        STRUCT('user' AS role, CONCAT('{system_prompt}', '\\n', prompt) AS content),\n",
    "        STRUCT('assistant' AS role, response AS content)\n",
    "    ) AS messages\n",
    "FROM hf_pubmed_eval;\n",
    "\"\"\")\n",
    "\n",
    "spark.table('hf_pubmed_eval_chat_complete').limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09f0810-86f5-467a-8d06-00ac691255b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start Fine Tuning Run\n",
    "\n",
    "See supported model types and more details on databricks' fine tuning module [here](https://docs.databricks.com/aws/en/large-language-models/foundation-model-training/).\n",
    "\n",
    "Notes on lower learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e7a370-eb44-4127-a39c-3ff6e37da984",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get Current Cluster ID"
    }
   },
   "outputs": [],
   "source": [
    "def get_current_cluster_id():\n",
    "  import json\n",
    "  return json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().safeToJson())['attributes']['clusterId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6624e911-0d23-413e-9b2e-0b981d56bbec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fine-tuning Run"
    }
   },
   "outputs": [],
   "source": [
    "run = fm.create(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                data_prep_cluster_id=get_current_cluster_id(), # necessary when using a Unity Catalog dataset to train\n",
    "                train_data_path=f\"{catalog}.{schema}.{train_table_name}\",\n",
    "                eval_data_path=f\"{catalog}.{schema}.{eval_table_name}\",\n",
    "                register_to=f\"{catalog}.{schema}.{model_name}\",\n",
    "                task_type=\"CHAT_COMPLETION\",\n",
    "                learning_rate=\"5e-8\",\n",
    "                training_duration=\"20ep\")\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e47dd9-0094-42ae-b31c-e89a62aeffad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add Alias To Latest Fine-Tuned Model Version"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get latest version\n",
    "versions = client.search_model_versions(f\"name='{catalog}.{schema}.{model_name}'\")\n",
    "latest_version = max(versions, key=lambda mv: int(mv.version)).version\n",
    "\n",
    "# Create or update the alias to point at the specified version\n",
    "alias = \"champion\"\n",
    "client.set_registered_model_alias(name=f\"{catalog}.{schema}.{model_name}\", alias=alias, version=latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7c44a7-4f6d-48c5-bb64-9e83a22d72c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Serving Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "061c740b-9451-4352-bb85-c6025804dc94",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Serving Endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import (\n",
    "    ServedEntityInput,\n",
    "    EndpointCoreConfigInput,\n",
    "    AiGatewayConfig,\n",
    "    AiGatewayInferenceTableConfig\n",
    ")\n",
    "\n",
    "serving_endpoint_name = \"cc-meta-llama-3-1-8b-instruct\"\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create the AI Gateway configuration\n",
    "ai_gateway_config = AiGatewayConfig(\n",
    "    inference_table_config=AiGatewayInferenceTableConfig(\n",
    "        enabled=True,\n",
    "        catalog_name=catalog,\n",
    "        schema_name=schema,\n",
    "        table_name_prefix=\"cc-meta-llama-3-1-8b-instruct_inference\"\n",
    "    )\n",
    ")\n",
    "\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    name=serving_endpoint_name,\n",
    "    served_entities=[\n",
    "        ServedEntityInput(\n",
    "            entity_name=registered_model_name,\n",
    "            entity_version=latest_version,\n",
    "            min_provisioned_throughput=0, # The minimum tokens per second that the endpoint can scale down to.\n",
    "            max_provisioned_throughput=10900,# The maximum tokens per second that the endpoint can scale up to. \n",
    "            scale_to_zero_enabled=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "existing_endpoint = next(\n",
    "    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None\n",
    ")\n",
    "\n",
    "if existing_endpoint is None:\n",
    "    print(f\"Creating the endpoint {serving_endpoint_name}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config, ai_gateway=ai_gateway_config)\n",
    "else:\n",
    "    print(f\"Endpoint {serving_endpoint_name} already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6613e31-997f-49e9-a0d5-d987e713eb4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inference with ai_query"
    }
   },
   "outputs": [],
   "source": [
    "test_table_name = \"hf_pubmed_test\"\n",
    "pred = spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            ai_query('{serving_endpoint_name}', concat('{system_prompt}', prompt)) AS prediction,\n",
    "            response,\n",
    "            prompt\n",
    "        FROM {catalog}.{schema}.{test_table_name}\n",
    "        \"\"\")\n",
    "display(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f419316b-7423-4a07-bd68-c67dd63ea60b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculate Fine-Tuned Accuracy"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate overall accuracy\n",
    "total_count = pred.count()\n",
    "correct_predictions = pred.filter(col(\"prediction\") == col(\"response\")).count()\n",
    "overall_accuracy = correct_predictions / total_count\n",
    "\n",
    "print(f\"Total samples: {total_count}\")\n",
    "print(f\"Finetuned Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46bfbaa0-b18a-4b88-a042-1436b323ad06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Our baseline accuracy was 40%, and our new accuracy after fine-tuning is 65%!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "finetune-llm-pubmed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
